========================================
⚠️ 重要声明
========================================
本软件开源于: https://github.com/TransWithAI/Faster-Whisper-TransWithAI-ChickenRice
开发团队: AI汉化组 (https://t.me/transWithAI)
========================================

基本用法:

将需要转录/翻译的音频或视频文件（或包含它们的文件夹）拖放到相应的批处理文件上运行。

=== 选择运行模式 ===

CPU模式:
- 拖放到 "运行(CPU).bat" - 使用CPU进行处理

GPU模式（仅限NVIDIA显卡）:
- 拖放到 "运行(GPU).bat" - 显存≥6GB时使用
- 拖放到 "运行(GPU,低显存模式).bat" - 显存4GB时使用
- 建议先更新显卡驱动到最新版本

视频专用模式:
- 拖放到 "运行(翻译视频)(CPU).bat" - 使用CPU处理视频
- 拖放到 "运行(翻译视频)(GPU).bat" - 使用GPU处理视频
- 拖放到 "运行(翻译视频)(GPU,低显存模式).bat" - 低显存GPU处理视频

输出到指定文件夹:
- 拖放到 "运行(GPU)(输出到当前文件夹).bat" - 字幕输出到"输出"文件夹

=== 支持的格式 ===

音频格式: mp3, wav, flac, m4a, aac, ogg, wma
视频格式: mp4, mkv, avi, mov, webm, flv, wmv

输出格式:
- LRC (歌词格式，适合音乐播放器)
- SRT (常用字幕格式，适合视频播放器)
- VTT (WebVTT格式，适合网页视频)

所有批处理文件默认会生成这三种格式的字幕。如果字幕文件已存在，将自动跳过。

---

调整参数:

1. 基本参数调整：
编辑批处理文件，找到以 "%cpath%\infer.exe" 开头的行，在其后添加参数。

示例（添加覆盖模式）:
添加前: "%cpath%\infer.exe" --device="cuda" %*
添加后: "%cpath%\infer.exe" --overwrite --device="cuda" %*

常用参数:
--overwrite : 覆盖已存在的字幕文件
--output_dir="路径" : 指定输出文件夹（默认输出到源文件所在文件夹）
--audio_suffixes="mp3,wav" : 自定义处理的文件格式
--sub_formats="srt,vtt,lrc" : 自定义输出格式
--log_level="INFO" : 减少控制台输出（默认为DEBUG）

2. 生成参数调整（高级）:
编辑 generation_config.json5 文件调整转录参数。
参数详情见下方相关项目链接。

补充：字幕合并/去重（segment_merge）
- 程序会尝试合并一些重复/重叠的识别片段，以减少“嗯嗯啊啊”等导致的重复字幕。
- 如果遇到“某一句字幕持续时间特别长（跨很久静音也不消失）”的情况，可在 generation_config.json5 里调整：
  - segment_merge.enabled: 关闭/开启合并
  - segment_merge.max_gap_ms: 只有当相邻两段间隔很短时才允许合并（建议 500~2000）
  - segment_merge.max_duration_ms: 单条字幕合并后的最大时长（建议 15000~30000）

注意：通常不需要调整生成参数。如遇到以下情况可尝试调整：
- 声音过小导致漏翻
- 时间轴对不上
- 出现幻听

---

故障排除:

1. GPU模式无法运行：
   - 确认是否为NVIDIA显卡
   - 更新显卡驱动到最新版本
   - 检查CUDA是否正确安装

2. 字幕未生成：
   - 检查文件格式是否支持
   - 查看控制台是否有错误信息
   - 尝试使用 --overwrite 参数重新生成

3. 内存不足：
   - 使用低显存模式
   - 尝试CPU模式
   - 处理较小的文件或分段处理

---

=== Modal 云端推理模式 ===

如果本地没有GPU或显存不足，可以使用Modal云端GPU进行推理。

1. 环境配置：

使用 Conda 创建轻量级环境（仅需 modal 和 questionary 库）：

```bash
conda env create -f environment-modal.yml
conda activate faster-whisper-modal
```

或手动安装：
```bash
pip install modal questionary
```

2. Modal 账号设置：

(1) 注册 Modal 账号：
   访问 https://modal.com/ 注册账号
   新用户每月有 $30 免费额度，足够处理大量音频

(2) 配置 Modal Token：
   在终端运行：
   ```bash
   modal token new
   ```
   按提示完成浏览器授权

(3) 配置 HuggingFace Token（可选但推荐）：

   为什么需要配置：
   - Modal 需要从 HuggingFace 下载模型文件
   - 某些模型需要登录才能下载
   - 配置后可避免下载失败或速度慢的问题

   如何配置：
   a. 获取 HF Token：
      访问 https://huggingface.co/settings/tokens
      创建一个 Read 权限的 token

   b. 在 Modal 中添加 Secret：
      访问 https://modal.com/secrets
      点击 "Create new secret"
      选择 "Hugging Face" 类型
      输入你的 HF Token
      保存为 "huggingface-secret"

3. 运行 Modal 推理：

```bash
python modal_infer.py
```

程序会交互式询问：
- 运行模式：一次性运行 或 持久化App
- GPU 类型：T4（推荐）、A10G、A100、H100 等
- 模型选择：基础版、海南鸡（日文转中文优化）、自定义模型
- 输入文件：本地音频文件或文件夹路径
- 批处理选项：是否启用批处理加速（需要更高显存）
- 超时时间：任务超时时间（分钟）

处理流程：
1. 上传本地音频文件到 Modal Volume
2. 在云端 GPU 上运行推理
3. 自动下载生成的字幕文件到本地
4. 输出结果保存在源文件同目录

注意事项：
- 首次运行需要构建 Docker 镜像，可能需要几分钟
- 模型首次下载会缓存在 Modal Volume，后续运行更快
- 推荐使用 T4 GPU，性价比最高
- 批处理模式可显著加速，但需要更多显存

---

相关项目：

- Faster Whisper: https://github.com/SYSTRAN/faster-whisper
- 海南鸡模型 (日文转中文优化): https://huggingface.co/chickenrice0721/whisper-large-v2-translate-zh-v0.2-st
- 音声优化 VAD 模型: https://huggingface.co/TransWithAI/Whisper-Vad-EncDec-ASMR-onnx
- OpenAI Whisper: https://github.com/openai/whisper
- 参数详情: https://github.com/SYSTRAN/faster-whisper/blob/dea24cbcc6cbef23ff599a63be0bbb647a0b23d6/faster_whisper/transcribe.py#L733

致谢：
- 基于 SYSTRAN/faster-whisper 开发
- 使用 chickenrice0721 日文转中文优化模型（5000小时音频数据训练）
- 使用 TransWithAI 音声优化 VAD 模型 (Whisper-Vad-EncDec-ASMR-onnx)
- 感谢某匿名群友的算力和技术支持
